{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOXQeHaEPR4cnU+bNeCKPM/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Dataset Loading**"],"metadata":{"id":"kULUFNUvcgO-"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","import re\n","\n","# Download NLTK resources\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Load the dataset\n","data = pd.read_csv(\"/content/spam_ham_dataset.csv\")\n","\n","# Exploratory Data Analysis (EDA)\n","print(\"Data Shape:\", data.shape)\n","print(\"Columns:\", data.columns)\n","print(\"Sample Data:\")\n","print(data.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MNDCzcPPfpi6","executionInfo":{"status":"ok","timestamp":1712643317759,"user_tz":-330,"elapsed":510,"user":{"displayName":"Malleswari Raavi","userId":"03692942386883129893"}},"outputId":"66de93e7-731f-46ae-aebc-97375524aedb"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Data Shape: (5171, 4)\n","Columns: Index(['Unnamed: 0', 'label', 'text', 'label_num'], dtype='object')\n","Sample Data:\n","   Unnamed: 0 label                                               text  \\\n","0         605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n","1        2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n","2        3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n","3        4685  spam  Subject: photoshop , windows , office . cheap ...   \n","4        2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n","\n","   label_num  \n","0          0  \n","1          0  \n","2          0  \n","3          1  \n","4          0  \n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["**Data Cleaning**"],"metadata":{"id":"N4JKSUEqf-Au"}},{"cell_type":"code","source":["data.drop_duplicates(inplace=True)\n","# Handle missing values\n","data.dropna(inplace=True)\n","# Text Cleaning\n","stop_words = set(stopwords.words('english'))\n","ps = PorterStemmer()\n","def clean_text(text):\n","    # Convert text to lowercase\n","    text = text.lower()\n","    # Remove special characters and digits\n","    text = re.sub(r'[^a-z\\s]', '', text)\n","    # Tokenize text\n","    tokens = word_tokenize(text)\n","    # Remove stopwords and apply stemming\n","    tokens = [ps.stem(word) for word in tokens if word not in stop_words]\n","    # Join tokens back into a single string\n","    cleaned_text = ' '.join(tokens)\n","    return cleaned_text"],"metadata":{"id":"I1NEZwj4gDWR","executionInfo":{"status":"ok","timestamp":1712643457115,"user_tz":-330,"elapsed":636,"user":{"displayName":"Malleswari Raavi","userId":"03692942386883129893"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["**Data Spliting**"],"metadata":{"id":"0k8lhP5HgYJ1"}},{"cell_type":"code","source":["data['cleaned_text'] = data['text'].apply(clean_text)\n","# Feature Engineering\n","# TF-IDF Vectorization\n","tfidf_vectorizer = TfidfVectorizer()\n","X = tfidf_vectorizer.fit_transform(data['cleaned_text'])\n","# Encoding target variable\n","label_encoder = LabelEncoder()\n","y = label_encoder.fit_transform(data['label'])\n","# Splitting the data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"metadata":{"id":"ACq67rlcgdJQ","executionInfo":{"status":"ok","timestamp":1712643565575,"user_tz":-330,"elapsed":24481,"user":{"displayName":"Malleswari Raavi","userId":"03692942386883129893"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["**Building the TensorFlow Model& Model Evalution**"],"metadata":{"id":"795SC2-Ngzju"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import scipy\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.metrics import accuracy_score\n","\n","# Load the dataset\n","data = pd.read_csv(\"/content/spam_ham_dataset.csv\")\n","\n","# Text preprocessing\n","tfidf_vectorizer = TfidfVectorizer()\n","X = tfidf_vectorizer.fit_transform(data['text'])\n","label_encoder = LabelEncoder()\n","y = label_encoder.fit_transform(data['label'])\n","\n","# Splitting the data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Build the TensorFlow model\n","model = Sequential([\n","    Dense(64, input_shape=(X_train.shape[1],), activation='relu'),\n","    Dropout(0.5),\n","    Dense(32, activation='relu'),\n","    Dropout(0.5),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","# Compile the model\n","model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n","print(\"Type of X_train:\", type(X_train))\n","print(\"Type of y_train:\", type(y_train))\n","print(\"Shape of X_train:\", X_train.shape)\n","print(\"Shape of y_train:\", y_train.shape)\n","if type(X_train) == scipy.sparse.csr.csr_matrix:\n","    X_train = X_train.toarray()\n","\n","if type(y_train) == np.ndarray:\n","    y_train = y_train.reshape(-1, 1)\n","model.fit(X_train, y_train, epochs=10, batch_size=32)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zcpX95WYhQOf","executionInfo":{"status":"ok","timestamp":1712645177333,"user_tz":-330,"elapsed":89185,"user":{"displayName":"Malleswari Raavi","userId":"03692942386883129893"}},"outputId":"2d89fa07-a481-4b21-f788-496667aca867"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Type of X_train: <class 'scipy.sparse._csr.csr_matrix'>\n","Type of y_train: <class 'numpy.ndarray'>\n","Shape of X_train: (4136, 50447)\n","Shape of y_train: (4136,)\n","Epoch 1/10\n","130/130 [==============================] - 11s 73ms/step - loss: 0.4195 - accuracy: 0.8172\n","Epoch 2/10\n","130/130 [==============================] - 7s 51ms/step - loss: 0.0649 - accuracy: 0.9886\n","Epoch 3/10\n","130/130 [==============================] - 8s 60ms/step - loss: 0.0206 - accuracy: 0.9966\n","Epoch 4/10\n","130/130 [==============================] - 7s 51ms/step - loss: 0.0069 - accuracy: 0.9998\n","Epoch 5/10\n","130/130 [==============================] - 9s 66ms/step - loss: 0.0042 - accuracy: 0.9998\n","Epoch 6/10\n","130/130 [==============================] - 9s 66ms/step - loss: 0.0025 - accuracy: 1.0000\n","Epoch 7/10\n","130/130 [==============================] - 7s 57ms/step - loss: 0.0018 - accuracy: 1.0000\n","Epoch 8/10\n","130/130 [==============================] - 8s 58ms/step - loss: 0.0018 - accuracy: 0.9998\n","Epoch 9/10\n","130/130 [==============================] - 7s 55ms/step - loss: 0.0011 - accuracy: 1.0000\n","Epoch 10/10\n","130/130 [==============================] - 8s 59ms/step - loss: 0.0011 - accuracy: 1.0000\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x78a2896b1390>"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["X_test = scipy.sparse.csr_matrix(X_test.toarray())\n","y_pred = model.predict(X_test)\n","from sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error, mean_absolute_error\n","\n","if type(y_test) == type(y_pred) == np.ndarray and np.isin(y_test, [0, 1]).all() and np.isin(y_pred, [0, 1]).all():\n","    accuracy = accuracy_score(y_test, y_pred)\n","elif type(y_test) == np.ndarray and np.isin(y_test, [0, 1]).all() and not np.isin(y_pred, [0, 1]).all():\n","    accuracy = roc_auc_score(y_test, y_pred)\n","else:\n","    accuracy = mean_squared_error(y_test, y_pred)\n","\n","print(\"Test Accuracy:\", accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jOpU4bFtmiqG","executionInfo":{"status":"ok","timestamp":1712645299260,"user_tz":-330,"elapsed":2473,"user":{"displayName":"Malleswari Raavi","userId":"03692942386883129893"}},"outputId":"efe8a0b6-0e0c-4192-8c82-ad521fa2df96"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["33/33 [==============================] - 0s 2ms/step\n","Test Accuracy: 0.9994526370017387\n"]}]},{"cell_type":"markdown","source":["**Analyzing Model Coefficients**"],"metadata":{"id":"_C46JF7KneAo"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","import matplotlib.pyplot as plt\n","\n","# Load the dataset\n","data = pd.read_csv(\"spam_ham_dataset.csv\")\n","\n","# Text preprocessing\n","tfidf_vectorizer = TfidfVectorizer()\n","X = tfidf_vectorizer.fit_transform(data['text'])\n","label_encoder = LabelEncoder()\n","y = label_encoder.fit_transform(data['label'])\n","\n","# Splitting the data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Build the TensorFlow model\n","model = Sequential([\n","    Dense(64, input_shape=(X_train.shape[1],), activation='relu'),\n","    Dropout(0.5),\n","    Dense(32, activation='relu'),\n","    Dropout(0.5),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n","print(\"Type of X_train:\", type(X_train))\n","print(\"Type of y_train:\", type(y_train))\n","print(\"Shape of X_train:\", X_train.shape)\n","print(\"Shape of y_train:\", y_train.shape)\n","if type(X_train) == scipy.sparse.csr.csr_matrix:\n","    X_train = X_train.toarray()\n","\n","if type(y_train) == np.ndarray:\n","    y_train = y_train.reshape(-1, 1)\n","model.fit(X_train, y_train, epochs=10, batch_size=32)\n","\n","# Extracting the weights of the first layer\n","weights_first_layer = model.layers[0].get_weights()[0]"],"metadata":{"id":"FsbkC4BOniM7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","print(\"Shape of X_train:\", X_train.shape)\n","print(\"Shape of weights_first_layer:\", weights_first_layer.shape)\n","if type(X_train) == scipy.sparse.csr.csr_matrix:\n","    X_train = X_train.toarray()\n","if type(y_train) == np.ndarray:\n","    y_train = y_train.reshape(-1, 1)\n","weights_first_layer = weights_first_layer.ravel()\n","plt.figure(figsize=(10, 6))\n","plt.bar(range(len(weights_first_layer)), weights_first_layer.ravel())\n","plt.xlabel('Feature Index')\n","plt.ylabel('Weight')\n","plt.title('Importance of Features (First Layer)')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ciJDLjxVpQ3Z","outputId":"3701e61f-4bf5-4036-e6be-468fdea20f5a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of X_train: (4136, 50447)\n","Shape of weights_first_layer: (50447, 64)\n"]}]}]}